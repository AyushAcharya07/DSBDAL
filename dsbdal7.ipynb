{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f296f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d79152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import (word_tokenize,sent_tokenize,TreebankWordTokenizer,wordpunct_tokenize,TweetTokenizer,MWETokenizer)\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cad0855",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"Success doesn't come from what you do occasionally but what you do consistently.\"\n",
    "text2=\"The future belongs to those who believe in the beauty of their dreams.\"\n",
    "text3=\"Hope, is the only thing stronger than fear! #Hope #Amal.M\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea86a5",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b234b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens in Text 1:  ['Success', 'does', \"n't\", 'come', 'from', 'what', 'you', 'do', 'occasionally', 'but', 'what', 'you', 'do', 'consistently', '.'] \n",
      "\n",
      "Word Tokens in Text 2:  ['The', 'future', 'belongs', 'to', 'those', 'who', 'believe', 'in', 'the', 'beauty', 'of', 'their', 'dreams', '.'] \n",
      "\n",
      "Word Tokens in Text 3:  ['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', '#', 'Hope', '#', 'Amal.M']\n"
     ]
    }
   ],
   "source": [
    "word_tokens1=word_tokenize(text1)\n",
    "word_tokens2=word_tokenize(text2)\n",
    "word_tokens3=word_tokenize(text3)\n",
    "print(\"Word Tokens in Text 1: \",word_tokens1,\"\\n\")\n",
    "print(\"Word Tokens in Text 2: \",word_tokens2,\"\\n\")\n",
    "print(\"Word Tokens in Text 3: \",word_tokens3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d5f6fe",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8384215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens in Text 1 :  [\"Success doesn't come from what you do occasionally but what you do consistently.\"] \n",
      "\n",
      "Sentence Tokens in Text 2 :  ['The future belongs to those who believe in the beauty of their dreams.'] \n",
      "\n",
      "Sentence Tokens in Text 3 :  ['Hope, is the only thing stronger than fear!', '#Hope #Amal.M']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens1=sent_tokenize(text1)\n",
    "print(\"Sentence Tokens in Text 1 : \",sentence_tokens1,\"\\n\")\n",
    "sentence_tokens2=sent_tokenize(text2)\n",
    "print(\"Sentence Tokens in Text 2 : \",sentence_tokens2,\"\\n\")\n",
    "sentence_tokens3=sent_tokenize(text3)\n",
    "print(\"Sentence Tokens in Text 3 : \",sentence_tokens3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82f2e7",
   "metadata": {},
   "source": [
    "### Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c012ba1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokens in Text 1 :  ['S', 'u', 'c', 'c', 'e', 's', 's', ' ', 'd', 'o', 'e', 's', 'n', \"'\", 't', ' ', 'c', 'o', 'm', 'e', ' ', 'f', 'r', 'o', 'm', ' ', 'w', 'h', 'a', 't', ' ', 'y', 'o', 'u', ' ', 'd', 'o', ' ', 'o', 'c', 'c', 'a', 's', 'i', 'o', 'n', 'a', 'l', 'l', 'y', ' ', 'b', 'u', 't', ' ', 'w', 'h', 'a', 't', ' ', 'y', 'o', 'u', ' ', 'd', 'o', ' ', 'c', 'o', 'n', 's', 'i', 's', 't', 'e', 'n', 't', 'l', 'y', '.'] \n",
      "\n",
      "Character Tokens in Text 2 :  ['T', 'h', 'e', ' ', 'f', 'u', 't', 'u', 'r', 'e', ' ', 'b', 'e', 'l', 'o', 'n', 'g', 's', ' ', 't', 'o', ' ', 't', 'h', 'o', 's', 'e', ' ', 'w', 'h', 'o', ' ', 'b', 'e', 'l', 'i', 'e', 'v', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'b', 'e', 'a', 'u', 't', 'y', ' ', 'o', 'f', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'd', 'r', 'e', 'a', 'm', 's', '.'] \n",
      "\n",
      "Character Tokens in Text 3 :  ['H', 'o', 'p', 'e', ',', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'o', 'n', 'l', 'y', ' ', 't', 'h', 'i', 'n', 'g', ' ', 's', 't', 'r', 'o', 'n', 'g', 'e', 'r', ' ', 't', 'h', 'a', 'n', ' ', 'f', 'e', 'a', 'r', '!', ' ', '#', 'H', 'o', 'p', 'e', ' ', '#', 'A', 'm', 'a', 'l', '.', 'M']\n"
     ]
    }
   ],
   "source": [
    "character_tokens1=list(text1)\n",
    "print(\"Character Tokens in Text 1 : \",character_tokens1,\"\\n\")\n",
    "character_tokens2=list(text2)\n",
    "print(\"Character Tokens in Text 2 : \",character_tokens2,\"\\n\")\n",
    "character_tokens3=list(text3)\n",
    "print(\"Character Tokens in Text 3 : \",character_tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3715bdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Word Tokens in Text 1 :  15 \n",
      "No. of Word Tokens in Text 2 :  14 \n",
      "No. of Word Tokens in Text 3 :  14\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Word Tokens in Text 1 : \",len(word_tokens1),\"\\nNo. of Word Tokens in Text 2 : \",len(word_tokens2),\"\\nNo. of Word Tokens in Text 3 : \",len(word_tokens3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10b72263",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Sentence Tokens in Text 1 :  1 \n",
      "No. of Sentence Tokens in Text 2 :  1 \n",
      "No. of Sentence Tokens in Text 3 :  2\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Sentence Tokens in Text 1 : \",len(sentence_tokens1),\"\\nNo. of Sentence Tokens in Text 2 : \",len(sentence_tokens2),\"\\nNo. of Sentence Tokens in Text 3 : \",len(sentence_tokens3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66d59329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Character Tokens in Text 1 :  80 \n",
      "No. of Character Tokens in Text 2 :  70 \n",
      "No. of Character Tokens in Text 3 :  57\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Character Tokens in Text 1 : \",len(character_tokens1 ),\"\\nNo. of Character Tokens in Text 2 : \",len(character_tokens2),\"\\nNo. of Character Tokens in Text 3 : \",len(character_tokens3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd4a81",
   "metadata": {},
   "source": [
    "### Punctuation Based Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1f964b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctutation based Tokens in Text 1 :  ['Success', 'doesn', \"'\", 't', 'come', 'from', 'what', 'you', 'do', 'occasionally', 'but', 'what', 'you', 'do', 'consistently', '.'] \n",
      "\n",
      "Punctutation based Tokens in Text 2 :  ['The', 'future', 'belongs', 'to', 'those', 'who', 'believe', 'in', 'the', 'beauty', 'of', 'their', 'dreams', '.'] \n",
      "\n",
      "Punctutation based Tokens in Text 3 :  ['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', '#', 'Hope', '#', 'Amal', '.', 'M'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "punct1=wordpunct_tokenize(text1)\n",
    "print(\"Punctutation based Tokens in Text 1 : \",punct1,\"\\n\")\n",
    "punct2=wordpunct_tokenize(text2)\n",
    "print(\"Punctutation based Tokens in Text 2 : \",punct2,\"\\n\")\n",
    "punct3=wordpunct_tokenize(text3)\n",
    "print(\"Punctutation based Tokens in Text 3 : \",punct3,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ade9b",
   "metadata": {},
   "source": [
    "### Treebank Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a3f82bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'you', 'do', \"n't\", 'want', 'to', 'be', 'done', 'to', 'yourself', ',', 'do', \"n't\", 'do', 'to', 'others', '...']\n"
     ]
    }
   ],
   "source": [
    "text4=\"What you don't want to be done to yourself, don't do to others...\"\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22607523",
   "metadata": {},
   "source": [
    "### Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7445c6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'take', 'cryptocurrency', 'advice', 'from', 'people', 'on', 'Twitter', 'üòÖ', 'üòÅ', 'üëå']\n"
     ]
    }
   ],
   "source": [
    "tweet=\"Don't take cryptocurrency advice from people on Twitter üòÖ üòÅ üëå\"\n",
    "tokenizer2=TreebankWordTokenizer()\n",
    "print(tokenizer2.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e5175",
   "metadata": {},
   "source": [
    "### MWE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "557a052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger_Games', '#', 'Hope', '.']\n"
     ]
    }
   ],
   "source": [
    "text5=\"Hope, is the only thing stronger than fear! Hunger Games #Hope.\"\n",
    "tokenizer3=MWETokenizer()\n",
    "tokenizer3.add_mwe((\"Hunger\",\"Games\"))\n",
    "print(tokenizer3.tokenize(word_tokenize(text5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8debd31c",
   "metadata": {},
   "source": [
    "### TextBlob Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82273a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', 'can', 'force', 'your', 'heart', 'and', 'nerve', 'and', 'sinew', 'to', 'serve', 'your', 'turn', 'long', 'after', 'they', 'are', 'gone', 'And', 'so', 'hold', 'on', 'when', 'there', 'is', 'nothing', 'in', 'you', 'except', 'the', 'Will', 'which', 'says', 'to', 'them', \"'Hold\", 'on']\n",
      "\n",
      "No.of words in Text 6 :  38\n"
     ]
    }
   ],
   "source": [
    "text6=\"If you can force your heart and nerve and sinew to serve your turn long after they are gone. And so hold on when there is nothing in you except the Will which says to them: 'Hold on!\"\n",
    "blob_object=TextBlob(text6)\n",
    "#Word Tokenization of text6\n",
    "text_words=blob_object.words\n",
    "print(text_words)\n",
    "print(\"\\nNo.of words in Text 6 : \",len(text_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1549608f",
   "metadata": {},
   "source": [
    "### Spacy Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "75827534",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22362/1173438749.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext7\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"All happy families are alike; each unhappy family is unhappy in its own way!!! üòá üëå üëå üëå #Leo Tolstoy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Token\\tToken ID\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "text7=\"All happy families are alike; each unhappy family is unhappy in its own way!!! üòá üëå üëå üëå #Leo Tolstoy\"\n",
    "nlp = spacy.blank(\"en\") \n",
    "doc=nlp(text7)\n",
    "print(\"Token\\tToken ID\\n\")\n",
    "for token in doc:\n",
    "    print(token,token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81f707",
   "metadata": {},
   "source": [
    "### Gensim Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516bc55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb463f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b036f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ebdb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c569537f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
