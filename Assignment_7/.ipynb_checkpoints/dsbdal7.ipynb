{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f296f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d5a50-a775-4bcd-b30b-44593e665323",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d79152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import (WhitespaceTokenizer,word_tokenize,sent_tokenize,TreebankWordTokenizer,wordpunct_tokenize,TweetTokenizer,MWETokenizer)\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cad0855",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"Success doesn't come from what you do occasionally but what you do consistently.\"\n",
    "text2=\"The future belongs to those who believe in the beauty of their dreams.\"\n",
    "text3=\"Hope, is the only thing stronger than fear! #Hope #Amal.M\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce0183-f366-404f-b5b9-9f3b041754cf",
   "metadata": {},
   "source": [
    "#### 1.1 Whitespace Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ba308e-613f-429a-9370-19a28d29d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk=WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962f4f55-fc51-4ab3-bb81-18bd72c13e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sun', 'shines', 'brightly', 'in', 'the', 'clear', 'blue', 'sky.', 'Birds', 'chirp', 'happily', 'as', 'they', 'fly', 'from', 'tree', 'to', 'tree.', 'The', 'sound', 'of', 'laughter', 'fills', 'the', 'air', 'as', 'children', 'play', 'in', 'the', 'park.']\n"
     ]
    }
   ],
   "source": [
    "text=\"The sun shines brightly in the clear blue sky. Birds chirp happily as they fly from tree to tree. The sound of laughter fills the air as children play in the park.\"\n",
    "df=tk.tokenize(text)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea86a5",
   "metadata": {},
   "source": [
    "#### 1.2 Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b234b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens in Text 1:  ['Success', 'does', \"n't\", 'come', 'from', 'what', 'you', 'do', 'occasionally', 'but', 'what', 'you', 'do', 'consistently', '.'] \n",
      "\n",
      "Word Tokens in Text 2:  ['The', 'future', 'belongs', 'to', 'those', 'who', 'believe', 'in', 'the', 'beauty', 'of', 'their', 'dreams', '.'] \n",
      "\n",
      "Word Tokens in Text 3:  ['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', '#', 'Hope', '#', 'Amal.M']\n"
     ]
    }
   ],
   "source": [
    "word_tokens1=word_tokenize(text1)\n",
    "word_tokens2=word_tokenize(text2)\n",
    "word_tokens3=word_tokenize(text3)\n",
    "print(\"Word Tokens in Text 1: \",word_tokens1,\"\\n\")\n",
    "print(\"Word Tokens in Text 2: \",word_tokens2,\"\\n\")\n",
    "print(\"Word Tokens in Text 3: \",word_tokens3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d5f6fe",
   "metadata": {},
   "source": [
    "#### 1.3 Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8384215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens in Text 1 :  [\"Success doesn't come from what you do occasionally but what you do consistently.\"] \n",
      "\n",
      "Sentence Tokens in Text 2 :  ['The future belongs to those who believe in the beauty of their dreams.'] \n",
      "\n",
      "Sentence Tokens in Text 3 :  ['Hope, is the only thing stronger than fear!', '#Hope #Amal.M']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens1=sent_tokenize(text1)\n",
    "print(\"Sentence Tokens in Text 1 : \",sentence_tokens1,\"\\n\")\n",
    "sentence_tokens2=sent_tokenize(text2)\n",
    "print(\"Sentence Tokens in Text 2 : \",sentence_tokens2,\"\\n\")\n",
    "sentence_tokens3=sent_tokenize(text3)\n",
    "print(\"Sentence Tokens in Text 3 : \",sentence_tokens3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82f2e7",
   "metadata": {},
   "source": [
    "#### 1.4 Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c012ba1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokens in Text 1 :  ['S', 'u', 'c', 'c', 'e', 's', 's', ' ', 'd', 'o', 'e', 's', 'n', \"'\", 't', ' ', 'c', 'o', 'm', 'e', ' ', 'f', 'r', 'o', 'm', ' ', 'w', 'h', 'a', 't', ' ', 'y', 'o', 'u', ' ', 'd', 'o', ' ', 'o', 'c', 'c', 'a', 's', 'i', 'o', 'n', 'a', 'l', 'l', 'y', ' ', 'b', 'u', 't', ' ', 'w', 'h', 'a', 't', ' ', 'y', 'o', 'u', ' ', 'd', 'o', ' ', 'c', 'o', 'n', 's', 'i', 's', 't', 'e', 'n', 't', 'l', 'y', '.'] \n",
      "\n",
      "Character Tokens in Text 2 :  ['T', 'h', 'e', ' ', 'f', 'u', 't', 'u', 'r', 'e', ' ', 'b', 'e', 'l', 'o', 'n', 'g', 's', ' ', 't', 'o', ' ', 't', 'h', 'o', 's', 'e', ' ', 'w', 'h', 'o', ' ', 'b', 'e', 'l', 'i', 'e', 'v', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'b', 'e', 'a', 'u', 't', 'y', ' ', 'o', 'f', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'd', 'r', 'e', 'a', 'm', 's', '.'] \n",
      "\n",
      "Character Tokens in Text 3 :  ['H', 'o', 'p', 'e', ',', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'o', 'n', 'l', 'y', ' ', 't', 'h', 'i', 'n', 'g', ' ', 's', 't', 'r', 'o', 'n', 'g', 'e', 'r', ' ', 't', 'h', 'a', 'n', ' ', 'f', 'e', 'a', 'r', '!', ' ', '#', 'H', 'o', 'p', 'e', ' ', '#', 'A', 'm', 'a', 'l', '.', 'M']\n"
     ]
    }
   ],
   "source": [
    "character_tokens1=list(text1)\n",
    "print(\"Character Tokens in Text 1 : \",character_tokens1,\"\\n\")\n",
    "character_tokens2=list(text2)\n",
    "print(\"Character Tokens in Text 2 : \",character_tokens2,\"\\n\")\n",
    "character_tokens3=list(text3)\n",
    "print(\"Character Tokens in Text 3 : \",character_tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3715bdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Word Tokens in Text 1 :  15 \n",
      "No. of Word Tokens in Text 2 :  14 \n",
      "No. of Word Tokens in Text 3 :  14\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Word Tokens in Text 1 : \",len(word_tokens1),\"\\nNo. of Word Tokens in Text 2 : \",len(word_tokens2),\"\\nNo. of Word Tokens in Text 3 : \",len(word_tokens3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10b72263",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Sentence Tokens in Text 1 :  1 \n",
      "No. of Sentence Tokens in Text 2 :  1 \n",
      "No. of Sentence Tokens in Text 3 :  2\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Sentence Tokens in Text 1 : \",len(sentence_tokens1),\"\\nNo. of Sentence Tokens in Text 2 : \",len(sentence_tokens2),\"\\nNo. of Sentence Tokens in Text 3 : \",len(sentence_tokens3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66d59329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Character Tokens in Text 1 :  80 \n",
      "No. of Character Tokens in Text 2 :  70 \n",
      "No. of Character Tokens in Text 3 :  57\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Character Tokens in Text 1 : \",len(character_tokens1 ),\"\\nNo. of Character Tokens in Text 2 : \",len(character_tokens2),\"\\nNo. of Character Tokens in Text 3 : \",len(character_tokens3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd4a81",
   "metadata": {},
   "source": [
    "#### 1.5 Punctuation Based Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1f964b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctutation based Tokens in Text 1 :  ['Success', 'doesn', \"'\", 't', 'come', 'from', 'what', 'you', 'do', 'occasionally', 'but', 'what', 'you', 'do', 'consistently', '.'] \n",
      "\n",
      "Punctutation based Tokens in Text 2 :  ['The', 'future', 'belongs', 'to', 'those', 'who', 'believe', 'in', 'the', 'beauty', 'of', 'their', 'dreams', '.'] \n",
      "\n",
      "Punctutation based Tokens in Text 3 :  ['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', '#', 'Hope', '#', 'Amal', '.', 'M'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "punct1=wordpunct_tokenize(text1)\n",
    "print(\"Punctutation based Tokens in Text 1 : \",punct1,\"\\n\")\n",
    "punct2=wordpunct_tokenize(text2)\n",
    "print(\"Punctutation based Tokens in Text 2 : \",punct2,\"\\n\")\n",
    "punct3=wordpunct_tokenize(text3)\n",
    "print(\"Punctutation based Tokens in Text 3 : \",punct3,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ade9b",
   "metadata": {},
   "source": [
    "#### 1.6 Treebank Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a3f82bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'you', 'do', \"n't\", 'want', 'to', 'be', 'done', 'to', 'yourself', ',', 'do', \"n't\", 'do', 'to', 'others', '...']\n"
     ]
    }
   ],
   "source": [
    "text4=\"What you don't want to be done to yourself, don't do to others...\"\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22607523",
   "metadata": {},
   "source": [
    "#### 1.7 Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7445c6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'take', 'cryptocurrency', 'advice', 'from', 'people', 'on', 'Twitter', '😅', '😁', '👌']\n"
     ]
    }
   ],
   "source": [
    "tweet=\"Don't take cryptocurrency advice from people on Twitter 😅 😁 👌\"\n",
    "tokenizer2=TreebankWordTokenizer()\n",
    "print(tokenizer2.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e5175",
   "metadata": {},
   "source": [
    "#### 1.8 MWE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "557a052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger_Games', '#', 'Hope', '.']\n"
     ]
    }
   ],
   "source": [
    "text5=\"Hope, is the only thing stronger than fear! Hunger Games #Hope.\"\n",
    "tokenizer3=MWETokenizer()\n",
    "tokenizer3.add_mwe((\"Hunger\",\"Games\"))\n",
    "print(tokenizer3.tokenize(word_tokenize(text5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8debd31c",
   "metadata": {},
   "source": [
    "#### 1.9 TextBlob Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82273a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', 'can', 'force', 'your', 'heart', 'and', 'nerve', 'and', 'sinew', 'to', 'serve', 'your', 'turn', 'long', 'after', 'they', 'are', 'gone', 'And', 'so', 'hold', 'on', 'when', 'there', 'is', 'nothing', 'in', 'you', 'except', 'the', 'Will', 'which', 'says', 'to', 'them', \"'Hold\", 'on']\n",
      "\n",
      "No.of words in Text 6 :  38\n"
     ]
    }
   ],
   "source": [
    "text6=\"If you can force your heart and nerve and sinew to serve your turn long after they are gone. And so hold on when there is nothing in you except the Will which says to them: 'Hold on!\"\n",
    "blob_object=TextBlob(text6)\n",
    "text_words=blob_object.words\n",
    "print(text_words)\n",
    "print(\"\\nNo.of words in Text 6 : \",len(text_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1549608f",
   "metadata": {},
   "source": [
    "#### 1.10 Spacy Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75827534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\tToken ID\n",
      "\n",
      "All 0\n",
      "happy 4\n",
      "families 10\n",
      "are 19\n",
      "alike 23\n",
      "; 28\n",
      "each 30\n",
      "unhappy 35\n",
      "family 43\n",
      "is 50\n",
      "unhappy 53\n",
      "in 61\n",
      "its 64\n",
      "own 68\n",
      "way 72\n",
      "! 75\n",
      "! 76\n",
      "! 77\n",
      "😇 79\n",
      "👌 81\n",
      "👌 83\n",
      "👌 85\n",
      "# 87\n",
      "Leo 88\n",
      "Tolstoy 92\n"
     ]
    }
   ],
   "source": [
    "text7=\"All happy families are alike; each unhappy family is unhappy in its own way!!! 😇 👌 👌 👌 #Leo Tolstoy\"\n",
    "nlp = spacy.blank(\"en\") \n",
    "doc=nlp(text7)\n",
    "print(\"Token\\tToken ID\\n\")\n",
    "for token in doc:\n",
    "    print(token,token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81f707",
   "metadata": {},
   "source": [
    "#### 1.11 Gensim Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "516bc55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All',\n",
       " 'happy',\n",
       " 'families',\n",
       " 'are',\n",
       " 'alike',\n",
       " 'each',\n",
       " 'unhappy',\n",
       " 'family',\n",
       " 'is',\n",
       " 'unhappy',\n",
       " 'in',\n",
       " 'its',\n",
       " 'own',\n",
       " 'way',\n",
       " 'Leo',\n",
       " 'Tolstoy']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "list(tokenize(text7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ce6cf-1735-48e3-a060-491778bce23a",
   "metadata": {},
   "source": [
    "#### 1.12 Tokenization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d6b036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "ntoken=Tokenizer(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90ebdb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'happy', 'families', 'are', 'alike', 'each', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '😇', '👌', '👌', '👌', 'leo', 'tolstoy']\n"
     ]
    }
   ],
   "source": [
    "ntoken.fit_on_texts(text7)\n",
    "list_words=text_to_word_sequence(text7)\n",
    "print(list_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741f398-c5e3-454d-b76a-76a29ab7b645",
   "metadata": {},
   "source": [
    "### 2. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0d1786f-88ea-44c9-8a79-433d0de17290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer,LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc45e663-aba2-4de0-8dc5-9627adb8b70d",
   "metadata": {},
   "source": [
    "#### 2.1 Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aa6a7f4-c2af-41dc-910a-7f37b54300f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c53966d8-1776-4d62-894c-ec1207c03874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run',\n",
       " 'quickli',\n",
       " 'through',\n",
       " 'the',\n",
       " 'forest,',\n",
       " 'the',\n",
       " 'deer',\n",
       " 'leap',\n",
       " 'over',\n",
       " 'fallen',\n",
       " 'branches.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_words(text):\n",
    "    word_tokens=text.split()\n",
    "    stems=[stem.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text8=\"Running quickly through the forest, the deer leaped over fallen branches.\"\n",
    "stem_words(text8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07305674-fe4a-412e-b49f-5b5e908ff8b3",
   "metadata": {},
   "source": [
    "#### 2.2 Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83b59ac8-224a-4445-b375-bb95a260913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f683276b-ba0a-461f-8c7f-f2fc3e3d2c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'bird', 'chirp', 'loud', 'as', 'they', 'flew', 'across', 'the', 'sky.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_words(text):\n",
    "    word_tokens=text.split()\n",
    "    stems=[snow.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text9=\"The birds chirped loudly as they flew across the sky.\"\n",
    "stem_words(text9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39807b10-632f-4bd9-8da7-e1aeaa7802ce",
   "metadata": {},
   "source": [
    "#### 2.3 Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ced44f92-1261-4d4f-8c25-9e0e8eb1ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "lan=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e091b37f-fd42-4699-a08c-cef57bb434c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she',\n",
       " 'pick',\n",
       " 'fresh',\n",
       " 'appl',\n",
       " 'from',\n",
       " 'the',\n",
       " 'orchard',\n",
       " 'to',\n",
       " 'mak',\n",
       " 'a',\n",
       " 'pie.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_words(text):\n",
    "    word_tokens=text.split()\n",
    "    stems=[lan.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text9=\"She picked fresh apples from the orchard to make a pie.\"\n",
    "stem_words(text9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4fab6-0935-4bf0-a5c9-d8d8d2cc6bba",
   "metadata": {},
   "source": [
    "### 3. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f4b6a3b-74c6-427e-86ed-458b6caf5d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9e5c0-66a3-4269-815c-189c515dd3a9",
   "metadata": {},
   "source": [
    "#### 3.1 WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8822590-8309-4864-a9b4-2354d2e78bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60a5b847-618c-4a1a-9942-5ade03087453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Running',\n",
       " 'quickly',\n",
       " 'through',\n",
       " 'the',\n",
       " 'forest,',\n",
       " 'the',\n",
       " 'deer',\n",
       " 'leaped',\n",
       " 'over',\n",
       " 'fallen',\n",
       " 'branches.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_words(text):\n",
    "    word_tokens=text.split()\n",
    "    stems=[lemmas.lemmatize(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text8=\"Running quickly through the forest, the deer leaped over fallen branches.\"\n",
    "stem_words(text8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b21d85-cf04-4cb3-ac23-5bcbdf724cf8",
   "metadata": {},
   "source": [
    "#### 3.2 Spacy Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9712b03-7ba3-4e3d-bed8-8c053db76080",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92a8be8b-5a23-4547-a587-6c7560c33141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run quickly through the forest , the deer leap over fall branch .\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_words(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_words = [token.lemma_ for token in doc]\n",
    "    lemmatized_text = \" \".join(lemmatized_words)\n",
    "    return lemmatized_text\n",
    "\n",
    "lemmatized_text = lemmatize_words(text8)\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99055d-a5f4-4dd2-9f76-513f6b963ef6",
   "metadata": {},
   "source": [
    "### 4. POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18d6d4e0-da32-4dc4-a121-ff188a2b25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7a6a476-a662-46da-8b63-2bb840981bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('She', 'PRP'), ('sells', 'VBZ'), ('seashells', 'NNS'), ('by', 'IN'), ('the', 'DT'), ('seashore', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text10=\"She sells seashells by the seashore.\"\n",
    "print(pos_tag(word_tokenize(text10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48bbcd38-c750-466f-a4d0-30b52feabdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('She', 'PRP'), ('sells', 'VBZ'), ('seashells', 'NNS'), ('by', 'IN'), ('the', 'DT'), ('seashore', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text10=\"She sells seashells by the seashore.\"\n",
    "print(pos_tag(word_tokenize(text10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff8e94ad-f29e-4718-9490-9438c18bfe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('intricate', 'JJ'), ('dance', 'NN'), ('performance', 'NN'), ('captivated', 'VBD'), ('the', 'DT'), ('audience', 'NN'), (',', ','), ('showcasing', 'VBG'), ('the', 'DT'), ('dancers', 'NNS'), (\"'\", 'POS'), ('agility', 'NN'), (',', ','), ('grace', 'NN'), (',', ','), ('and', 'CC'), ('precision', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text11=\"The intricate dance performance captivated the audience, showcasing the dancers' agility, grace, and precision.\"\n",
    "print(pos_tag(word_tokenize(text11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48bd08be-2ec1-4335-9055-a4c7d51098d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Peter', 'NNP'), ('Piper', 'NNP'), ('picked', 'VBD'), ('a', 'DT'), ('peck', 'NN'), ('of', 'IN'), ('pickled', 'JJ'), ('peppers', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text12=\"Peter Piper picked a peck of pickled peppers.\"\n",
    "print(pos_tag(word_tokenize(text12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ba999-3215-4b4b-a91c-044dd9fdc4e5",
   "metadata": {},
   "source": [
    "#### 5. TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8face5e6-c7ac-41c2-914d-e71c9ba17c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentA = 'Jupiter is the largest Planet'\n",
    "documentB = 'Mars is the fourth planet from the Sun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab0286d8-7e8d-4e7d-be67-b20415f0e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dbdadc96-7629-48d3-9028-08496f8e81f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jupiter', 'is', 'the', 'largest', 'Planet']\n"
     ]
    }
   ],
   "source": [
    "print(bagOfWordsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "425177e2-e30d-4bdb-ac50-44922f3a567c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'largest', 'Mars', 'from', 'the', 'Planet', 'is', 'Sun', 'planet', 'Jupiter', 'fourth'}\n"
     ]
    }
   ],
   "source": [
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
    "print(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2141b7d1-cf18-4c1f-9bcf-179304ec4797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'largest': 1, 'Mars': 0, 'from': 0, 'the': 1, 'Planet': 1, 'is': 1, 'Sun': 0, 'planet': 0, 'Jupiter': 1, 'fourth': 0}\n"
     ]
    }
   ],
   "source": [
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1\n",
    "print(numOfWordsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd2d27b8-d1f3-4245-8453-251a3147023c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'largest': 0.0,\n",
       " 'Mars': 0.125,\n",
       " 'from': 0.125,\n",
       " 'the': 0.25,\n",
       " 'Planet': 0.0,\n",
       " 'is': 0.125,\n",
       " 'Sun': 0.125,\n",
       " 'planet': 0.125,\n",
       " 'Jupiter': 0.0,\n",
       " 'fourth': 0.125}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
    "tfA\n",
    "tfB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fcec919b-8ddc-4f03-a01b-a641d92ee729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'largest': 0.6931471805599453,\n",
       " 'Mars': 0.6931471805599453,\n",
       " 'from': 0.6931471805599453,\n",
       " 'the': 0.0,\n",
       " 'Planet': 0.6931471805599453,\n",
       " 'is': 0.0,\n",
       " 'Sun': 0.6931471805599453,\n",
       " 'planet': 0.6931471805599453,\n",
       " 'Jupiter': 0.6931471805599453,\n",
       " 'fourth': 0.6931471805599453}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict\n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df29f294-a5c8-4928-a8fa-a45baf5431b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>largest</th>\n",
       "      <th>Mars</th>\n",
       "      <th>from</th>\n",
       "      <th>the</th>\n",
       "      <th>Planet</th>\n",
       "      <th>is</th>\n",
       "      <th>Sun</th>\n",
       "      <th>planet</th>\n",
       "      <th>Jupiter</th>\n",
       "      <th>fourth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    largest      Mars      from  the    Planet   is       Sun    planet  \\\n",
       "0  0.138629  0.000000  0.000000  0.0  0.138629  0.0  0.000000  0.000000   \n",
       "1  0.000000  0.086643  0.086643  0.0  0.000000  0.0  0.086643  0.086643   \n",
       "\n",
       "    Jupiter    fourth  \n",
       "0  0.138629  0.000000  \n",
       "1  0.000000  0.086643  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3183652-7477-4866-985e-9c9f5e7e94a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5598bc-a6ee-4840-a560-fe55a6e68a25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
